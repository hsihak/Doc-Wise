# -*- coding: utf-8 -*-
"""Phase 1_Senmantic similarity comparison.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_AzaSxBSS7DSReiDX-ghdho6Mk1L_NKA
"""

# Import Libraries for file processing and text analysis.
from google.colab import files
import os
import textract
import PyPDF2
import subprocess
from docx import Document
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import csv

# Load pre-trained BERT model and tokenizer from Hugging Face's Transformers library.
# BERT is a state-of-the-art language understanding model used for text embedding.
from transformers import BertModel, BertTokenizer
import torch

model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Function to encode text using the BERT model. This is used to convert text to a numerical form (embeddings)
# that represents the semantic meaning of the text.
def encode(text):
    # Convert text to BERT's format (token ids), with a max length of 512 tokens.
    input_ids = torch.tensor(tokenizer.encode(text, truncation=True, max_length=512)).unsqueeze(0)  # Batch size 1
    outputs = model(input_ids)
    # Use the mean of the last layer's hidden states as the sentence representation.
    sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1).detach().numpy()
    return sentence_embedding


# NLTK is a leading platform for building Python programs to work with human language data.
# Download necessary NLTK resources for text preprocessing: stopwords and lemmatizer.
import nltk
nltk.download('stopwords')
nltk.download('wordnet')

# File upload functionality, typically for front-end users to upload documents.
uploaded = files.upload()

# Initialize a set of stopwords and a lemmatizer for text preprocessing.
# Stopwords are common words (like 'the', 'a', 'in') which are usually ignored in text processing.
# Lemmatization is the process of reducing words to their base or root form (e.g., 'running' to 'run').
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Creating a temporary directory to store uploaded files.
temp_dir = "/content/temp_files/"
os.makedirs(temp_dir, exist_ok=True)

temp_file_paths = []
uploaded_files = list(uploaded.keys())
for file_name in uploaded_files:
    file_content = uploaded[file_name]
    temp_file_path = os.path.join(temp_dir, file_name)
    with open(temp_file_path, "wb") as f:
        f.write(file_content)
    temp_file_paths.append(temp_file_path)

preprocessed_texts = []
# Processing each file based on its format (DOCX, DOC, PDF, etc.) and extracting text from it.
for file_path in temp_file_paths:
    if file_path.endswith(".docx"):
        # Reading DOCX file.
        doc = Document(file_path)
        text = [paragraph.text for paragraph in doc.paragraphs]
        preprocessed_texts.append(" ".join(text))

    elif file_path.endswith(".doc"):
        # Converting DOC file to DOCX format.
        docx_file_path = file_path + "x"
        subprocess.call(['soffice', '--headless', '--convert-to', 'docx', file_path])

        # Reading the converted DOCX file.
        doc = Document(docx_file_path)
        text = [paragraph.text for paragraph in doc.paragraphs]
        preprocessed_texts.append(" ".join(text))

        # Removing temporary .docx file.
        os.remove(docx_file_path)

    elif file_path.endswith(".pdf"):
        # Reading PDF file.
        pdf_reader = PyPDF2.PdfReader(file_path)
        text = ""
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text += page.extract_text()
        preprocessed_texts.append(text)

    else:
        # Reading other file types using textract.
        text = textract.process(file_path, method='text').decode('utf-8')
        preprocessed_texts.append(text)

# Applying preprocessing: removing stopwords and lemmatizing the text.
# This is done to reduce the text to its most informative components.
preprocessed_texts = [" ".join([lemmatizer.lemmatize(word.lower()) for word in text.split() if word.lower() not in stop_words]) for text in preprocessed_texts]

# TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word
# to a document in a collection or corpus. Here, it's used to convert the text to a numerical form.
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(preprocessed_texts).toarray()

# Converting the preprocessed texts into BERT embeddings for semantic analysis.
bert_embeddings = np.array([encode(text)[0] for text in preprocessed_texts])

# Computing cosine similarity separately for TF-IDF and BERT vectors.
# Cosine similarity measures the cosine of the angle between two non-zero vectors of an inner product space,
# which is used as a measure of similarity between two vectors.
tfidf_similarity_scores = cosine_similarity(tfidf_matrix)
bert_similarity_scores = cosine_similarity(bert_embeddings)

# Averaging the similarity scores from both TF-IDF and BERT to get a more comprehensive similarity measure.
average_similarity_scores = np.mean([tfidf_similarity_scores, bert_similarity_scores], axis=0)

# Displaying the average similarity scores for visualization.
print("\nAverage Similarity Scores:")
print(average_similarity_scores)

# Create a DataFrame for better visualization
average_similarity_df = pd.DataFrame(average_similarity_scores, index=uploaded_files, columns=uploaded_files)

# Save the dataframe to an Excel file
average_similarity_df.to_excel('similarity_scores.xlsx')

# Function to output similarity scores in a threshold-based CSV format.
def output_to_csv(similarity_scores, uploaded_files, output_csv_path):
    # Prepare the data with different similarity thresholds.
    data = [["Threshold", "File Pair's Names"], ["Similarity between files > 90%"], ["Similarity between files > 80%"], ["Similarity between files > 70%"], ["Similarity between files > 60%"], ["Similarity between files > 50%"], ["Similarity between files <= 50%"]]

    for i in range(len(uploaded_files)):
        for j in range(i+1, len(uploaded_files)):
            file1 = uploaded_files[i]
            file2 = uploaded_files[j]
            similarity_score = similarity_scores[i][j]
            similarity_score = round(similarity_score * 100, 2)

            # Categorizing file pairs based on their similarity percentage.
            if similarity_score > 90:
                data[1].append(f"{file1} and {file2}")
            elif similarity_score > 80:
                data[2].append(f"{file1} and {file2}")
            elif similarity_score > 70:
                data[3].append(f"{file1} and {file2}")
            elif similarity_score > 60:
                data[4].append(f"{file1} and {file2}")
            elif similarity_score > 50:
                data[5].append(f"{file1} and {file2}")
            else:
                data[6].append(f"{file1} and {file2}")

    # Writing the similarity data to a CSV file.
    # The CSV file will be useful for front-end applications to display or use the similarity data.
    with open(output_csv_path, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(data)

# Compute cosine similarity separately for TF-IDF and BERT vectors.
tfidf_similarity_scores = cosine_similarity(tfidf_matrix)
bert_similarity_scores = cosine_similarity(bert_embeddings)

# Average the similarity scores from both TF-IDF and BERT models.
average_similarity_scores = np.mean([tfidf_similarity_scores, bert_similarity_scores], axis=0)

# Call the function to output similarity scores to a CSV file.
# This CSV file can be used by the front-end for displaying similarity results or for further processing.
output_csv_path = "threshold_similarity_scores.csv"
output_to_csv(average_similarity_scores, uploaded_files, output_csv_path)

"""**Additional Notes for Frontend Integration:**

- *File Upload Interface*: Ensure the front-end allows multiple file uploads and handles different file types (PDF, DOCX) as covered in the script - please ignore .DOC for now.

- *Displaying Results*: The similarity scores are saved in CSV files. The front-end can present these as downloadable links or display the contents directly on the webpage.

- *Feedback Loop (optional, for future reference)*: Provide an option for users to give feedback on the similarity results. This can be used to further refine the models and thresholds.

- *Accessibility and Performance*: Make sure the front-end is user-friendly, accessible, and performs efficiently, especially when handling large files or multiple uploads.
"""